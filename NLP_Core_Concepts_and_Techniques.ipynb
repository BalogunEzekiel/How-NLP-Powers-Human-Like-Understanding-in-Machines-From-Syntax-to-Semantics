{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f503916-f1d2-418e-8230-1e088433f534",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Core Concepts and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb862c33-fd9c-4113-a38b-f50472fc47d6",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down a stream of text into smaller units, usually words or subwords. These tokens form the base unit for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e5724-7d4c-4596-9dd7-86d75a8cf505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3243ccf6-3ab7-4a45-96d0-7ceadf67ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Natural Language Processing is fascinating.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14ecd9-f00a-46a1-ae41-252b1e8569d3",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n",
    "\n",
    "Stop words are common words (like \"and\", \"the\", \"in\") that are often removed from text data during preprocessing to reduce noise and focus on more meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454a799f-5b5d-44d1-bbe1-4922ed9daf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45d5fa0-5143-4ed7-a012-a2208b462aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['This', 'is', 'a', 'simple', 'example', 'showing', 'how', 'stop', 'words', 'are', 'removed', 'from', 'text', '.']\n",
      "Without Stop Words: ['simple', 'example', 'showing', 'stop', 'words', 'removed', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is a simple example showing how stop words are removed from text.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"Without Stop Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ef89e8-3a47-42f9-8bec-a26ca01abf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "sentence = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Filter stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696abc04-3020-4058-94b5-765da8f5b013",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Both stemming and lemmatization are fundamental techniques in natural language processing (NLP) used to normalize text and reduce words to their base or root forms, thereby improving the performance of text analysis and machine learning models.\n",
    "\n",
    "* ***Stemming***\n",
    "\n",
    "Stemming involves the process of chopping off the ends of words to arrive at the root form. It uses heuristic rules to strip suffixes, which can sometimes lead to non-dictionary words.\n",
    "* ***Lemmatization***\n",
    "\n",
    "Lemmatization is a more sophisticated method that uses vocabulary and morphological analysis to return the base or dictionary form of a word, known as the lemma. It considers the part of speech (POS) of a word, which leads to more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d51929a-b575-455d-a457-09538b82dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a128680-07ee-4520-9797-ceadb8f5fd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word       Stemmed    Lemmatized  \n",
      "-----------------------------------\n",
      "running    run        run         \n",
      "flies      fli        fly         \n",
      "better     better     better      \n",
      "eating     eat        eat         \n",
      "cats       cat        cat         \n",
      "studies    studi      study       \n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample words to test\n",
    "words = [\"running\", \"flies\", \"better\", \"eating\", \"cats\", \"studies\"]\n",
    "\n",
    "# Compare stemming and lemmatization\n",
    "print(f\"{'Word':<10} {'Stemmed':<10} {'Lemmatized':<12}\")\n",
    "print(\"-\" * 35)\n",
    "for word in words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos=\"v\")  # pos=\"v\" for verb\n",
    "    print(f\"{word:<10} {stemmed:<10} {lemmatized:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788d6e1-8782-465e-9d33-dbd99198a7dc",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging\n",
    "\n",
    "This technique assigns parts of speech (nouns, verbs, adjectives, etc.) to each token, crucial for syntactic understanding.\n",
    "\n",
    "* JJ:   Adjective\n",
    "\n",
    "* NN:   Noun (singular)\n",
    "\n",
    "* VBZ:   Verb, 3rd person singular present\n",
    "\n",
    "* VBG:   Verb, gerund or present participle\n",
    "\n",
    "* .        :   Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "549d51e0-3a59-435c-bf16-784bb80c681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Downloads\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize and tag\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Output\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cb12a54-01a4-470d-99c6-3ff48449d200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags:\n",
      "John       → NNP\n",
      "is         → VBZ\n",
      "playing    → VBG\n",
      "football   → NN\n",
      "in         → IN\n",
      "the        → DT\n",
      "park       → NN\n",
      "with       → IN\n",
      "his        → PRP$\n",
      "friends    → NNS\n",
      ".          → .\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "sentence = \"John is playing football in the park with his friends.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Get part-of-speech tags\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"POS Tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word:10} → {tag}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
