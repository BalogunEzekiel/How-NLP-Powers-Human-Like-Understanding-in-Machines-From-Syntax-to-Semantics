{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e907c99d-5f29-4a5f-985d-f428d9c97654",
   "metadata": {},
   "source": [
    "## NLP PROJECTS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699903af-9823-400c-968e-ce41d0e2b030",
   "metadata": {},
   "source": [
    "### Introduction to Natural Language Processing (NLP)\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence (AI) that enables computers to understand, interpret and generate human language. It involves a wide range of computational techniques that allow machines to process spoken and written languages, making it possible for computers to communicate effectively with humans.\n",
    "\n",
    "NLP bridges the gap between human communication and computer understanding. It plays a crucial role in various applications such as:\n",
    "\n",
    "Machine Translation (e.g Google Translate)\n",
    "\n",
    "Speech Recognition (e.g Siri, Alexa)\n",
    "\n",
    "Text Summarization\n",
    "\n",
    "Sentiment Analysis\n",
    "\n",
    "Chatbots and Virtual Assistants\n",
    "\n",
    "Spam Detection\n",
    "\n",
    "Information Retrieval (e.g search engines)\n",
    "\n",
    "\n",
    "**Key components of NLP include:**\n",
    "\n",
    "Tokenization ‚Äì breaking text into words or phrases\n",
    "\n",
    "Part-of-Speech Tagging ‚Äì identifying nouns, verbs, adjectives, etc.\n",
    "\n",
    "Named Entity Recognition (NER) ‚Äì extracting names of people, places and organizations\n",
    "\n",
    "Parsing ‚Äì analyzing grammatical structure\n",
    "\n",
    "Semantic Analysis ‚Äì understanding meaning and context\n",
    "\n",
    "With the advancement of deep learning and large language models, modern NLP systems have become more accurate and powerful, enabling real-time translation, content generation and more human-like conversations. NLP is now an integral part of technologies we use every day and continues to evolve rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba152e-5a54-4e25-82b9-7ef874d3c242",
   "metadata": {},
   "source": [
    "### Five Significance of NLP Applications.\n",
    "\n",
    "**(i) Chatbots and Virtual Assistants (e.g., Siri, Google Assistant)**\n",
    "NLP allows chatbots and virtual assistants to understand user queries and provide relevant responses. This enhances customer support, automates tasks, and improves overall user experience.\n",
    "\n",
    "**(ii) Sentiment Analysis (e.g., Social Media Monitoring, Customer Feedback Analysis)**\n",
    "Businesses use NLP to analyze customer feedback and social media posts to understand public sentiment. This helps companies improve their services, respond to customer concerns, and manage their reputation.\n",
    "\n",
    "**(iii) Machine Translation (e.g., Google Translate, DeepL)**\n",
    "NLP enables the translation of text from one language to another, breaking language barriers and making global communication more accessible. This is crucial for international business, education, and cross-cultural exchange.\n",
    "\n",
    "**(iv) Information Extraction (e.g., Medical Reports, Legal Documents)**\n",
    "NLP can automatically extract important information from unstructured text, such as identifying key entities, dates, and relationships in medical or legal documents. This saves time, reduces human error, and aids in faster decision-making.\n",
    "\n",
    "**(v) Text Summarization (e.g., News Aggregators, Research Papers)**\n",
    "NLP is used to generate concise summaries of long texts, making it easier to digest large volumes of information. Applications range from summarizing news articles to condensing academic papers or legal documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef001776-45cb-489d-86da-3d6801b30756",
   "metadata": {},
   "source": [
    "### Five Challenges of NLP.\n",
    "\n",
    "**(i) Noisy Data**\n",
    "NLP models rely on large amounts of text data, but real-world data is often messy. It may contain typos, slang, abbreviations, grammatical errors or inconsistencies. For example, social media posts often have informal language and emojis, making it harder for NLP systems to process them accurately.\n",
    "\n",
    "**(ii) Context Dependency**\n",
    "Understanding language requires considering the context in which words or phrases appear. The same words can have different meanings depending on prior conversations, cultural background, or even tone. E.g., \"I was at the bank when the rain started\" ‚Äì the word \"bank\" could mean a financial institution or a riverbank, depending on the speaker‚Äôs intent and/or context.\n",
    "\n",
    "**(iii) Ambiguity and Polysemy**\n",
    "Many words have multiple meanings (polysemy), and sentences can often be interpreted in more than one way (ambiguity). For example, \"She saw the man with the telescope\" could mean she used a telescope to see the man, or the man had a telescope. Resolving such ambiguities is a major challenge in NLP.\n",
    "\n",
    "**(iv) Sarcasm and Irony**\n",
    "Detecting sarcasm and irony requires an understanding that goes beyond literal meanings. Statements like ‚ÄúOh great, another Monday‚Äù can appear positive on the surface but are often meant sarcastically. NLP systems often struggle to detect these nuances, especially without vocal tone or facial cues.\n",
    "\n",
    "**(v) Low-Resource Languages**\n",
    "While popular languages like English have vast amounts of training data, many languages lack sufficient labeled datasets and linguistic resources. This creates a performance gap in NLP applications across different languages, limiting accessibility and inclusiveness in global AI solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74846164-c582-469e-bb18-f790f1631644",
   "metadata": {},
   "source": [
    "# Hands-On Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18350d-1a07-487c-80d7-28531622c9fa",
   "metadata": {},
   "source": [
    "### Key Word Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1762e03e-3148-40d8-af4e-c2fcea0b59b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails Found: ['support@company.com', 'sales@business.org', 'info@service.net']\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import re\n",
    "\n",
    "# Sentence to process\n",
    "sentence = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net.\"\n",
    "\n",
    "# Extract emails\n",
    "emails = re.findall(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b', sentence)\n",
    "\n",
    "# Print all emails\n",
    "print('Emails Found:', emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3d0a67-d3d5-40d6-be85-80394b0b168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing', 'cleaning', 'processing', 'learning']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence to process\n",
    "sentence = \"NLP is amazing for cleaning processing text while learning new techniques.\"\n",
    "\n",
    "# Find words ending with 'ing'\n",
    "ing_words = re.findall(r\"\\b\\w+ing\\b\", sentence)\n",
    "ing_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc887f-8e55-48e2-88cd-ef4e20561cd8",
   "metadata": {},
   "source": [
    "### Text Cleaning (preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54bdfb93-45f0-4a95-8935-1022a9d3dada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP makes AI smarter But sometimes its challenging Dont you agree'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to process\n",
    "text = \"NLP makes AI smarter! But, sometimes, it's challenging... Don't you agree?\"\n",
    "\n",
    "# Remove all punctuation\n",
    "no_punctuation_text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "# Print result\n",
    "no_punctuation_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f2bcb7-889e-40cc-84a2-3451ab726da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp makes ai smarter but sometimes its challenging dont you agree'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to lowercase\n",
    "no_punctuation_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a26691-1318-49f3-8f05-8ea5a0636bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'makes',\n",
       " 'AI',\n",
       " 'smarter',\n",
       " 'But',\n",
       " 'sometimes',\n",
       " 'its',\n",
       " 'challenging',\n",
       " 'Dont',\n",
       " 'you',\n",
       " 'agree']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split words\n",
    "split_words = re.split(r'\\s+', no_punctuation_text)\n",
    "split_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2932318e-4e0b-425a-9760-e0426ac405e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg nlp is so cool it costs 1000 learn it now\n"
     ]
    }
   ],
   "source": [
    "# Text to process\n",
    "text = \"OMG!! NLP is soooo coool ü§©...!!! It costs $1000. Learn it now at https://3mtt.com¬†üòé.\"\n",
    "\n",
    "\n",
    "# Text cleaning (preprocess)\n",
    "text = text.lower()  # Lowercase\n",
    "text = re.sub(r'at https?://\\S+', '', text)  # Remove at and URLs\n",
    "text = re.sub(r'\\$(\\d+)', '1000', text)  # Replace $1000 ‚Üí 1000\n",
    "text = re.sub(r'([a-zA-Z])\\1{2,}', r'\\1', text)  # Reduce repeated characters\n",
    "text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation, symbols and emoji's\n",
    "text = re.sub(r'\\bcol\\b', 'cool', text)  # Fix: replace col ‚Üí cool\n",
    "text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "\n",
    "# Print result\n",
    "cleaned_text = text\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648b29-07f6-4b86-8b84-e09d58d4c857",
   "metadata": {},
   "source": [
    "### Tokenization (word_level & sentence_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7443a11-5e24-4fa7-9ff7-69d8ddb435d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word-Level Tokenization:\n",
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.', 'It', 'splits', 'text', 'into', 'smaller', 'pieces', 'for', 'analysis', '.']\n",
      "\n",
      "Sentence-Level Tokenization:\n",
      "['Tokenization is the first step in NLP.', 'It splits text into smaller pieces for analysis.']\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Assign variable\n",
    "text = \"Tokenization is the first step in NLP. It splits text into smaller pieces for analysis.\"\n",
    "\n",
    "# Apply Word-level tokenization\n",
    "words = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Print result\n",
    "print(\"\\nWord-Level Tokenization:\")\n",
    "print(words)\n",
    "print(\"\\nSentence-Level Tokenization:\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8881c-fbb9-4bc7-868a-06a65e3953f7",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "* Porter Stemmer for stemming.\n",
    "* spaCy for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb9ac47-b9b7-466f-8460-42b3757e3eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['run', 'file', 'studi', 'easili', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Words to process\n",
    "words = [\"running\", \"files\", \"studies\", \"easily\", \"better\"]\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print result\n",
    "print(\"Porter Stemmer:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a414d95-31b8-41c7-a633-aab1bf487bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Lemmatizer: ['run', 'file', 'study', 'easily', 'well']\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Words to process\n",
    "words = [\"running\", \"files\", \"studies\", \"easily\", \"better\"]\n",
    "\n",
    "# Apply lemmatization with spaCy\n",
    "doc = nlp(\" \".join(words))\n",
    "lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "# Print result\n",
    "print(\"spaCy Lemmatizer:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29708773-8073-4c1b-abdd-71021ab66a79",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d8b5c0-b1d5-4ae6-9093-4ee4379ec0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded vectors:\n",
      "apple: [1, 0, 0, 0, 0]\n",
      "banana: [0, 1, 0, 0, 0]\n",
      "cherry: [0, 0, 1, 0, 0]\n",
      "mango: [0, 0, 0, 1, 0]\n",
      "Orange: [0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import numpy\n",
    "\n",
    "# Vocabulary\n",
    "vocab = [\"apple\", \"banana\", \"cherry\", \"mango\", \"Orange\"]\n",
    "\n",
    "# Create a dictionary mapping each word to its one-hot vector\n",
    "one_hot_vectors = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    # Create a zero vector of length equal to the vocabulary size\n",
    "    vector = [0] * len(vocab)\n",
    "    # Set the position for the current word to 1\n",
    "    vector[i] = 1\n",
    "    # Store in the dictionary\n",
    "    one_hot_vectors[word] = vector\n",
    "\n",
    "# Print the one-hot encoded vectors\n",
    "print(\"One-hot encoded vectors:\")\n",
    "for word, vector in one_hot_vectors.items():\n",
    "    print(f\"{word}: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec206c4-4b5f-4d15-b7e4-ce528e30441c",
   "metadata": {},
   "source": [
    "### Bag of Words using CountVectorizer & TF-IDF using TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c089b20-8d80-4243-9d97-6a34e6e3292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['brown' 'dog' 'fox' 'in' 'jumbs' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
      " 'the']\n",
      "Bag of Words Matrix:\n",
      " [[1 1 1 0 1 0 1 1 1 0 2]\n",
      " [0 1 0 1 0 1 0 0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Dataset\n",
    "sentences = [\"The quick brown fox jumbs over the lazy dog.\",\n",
    "             \"The dog sleeps in the kernel.\"]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Convert to array and display\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e36d96-fcd2-41d5-8ffe-7a05942f518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['brown' 'dog' 'fox' 'in' 'jumbs' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
      " 'the']\n",
      "TF-IDF Matrix:\n",
      " [[0.342369   0.24359836 0.342369   0.         0.342369   0.\n",
      "  0.342369   0.342369   0.342369   0.         0.48719673]\n",
      " [0.         0.30253071 0.         0.42519636 0.         0.42519636\n",
      "  0.         0.         0.         0.42519636 0.60506143]]\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Convert to array and display\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531bcde-4295-4a18-87d0-df351f39dca9",
   "metadata": {},
   "source": [
    "### Word2Vec Model Using Gensim\n",
    "\n",
    "* **Retreive embedding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5894d92c-14ac-477a-afc4-86a7882591b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'snake':\n",
      "[ 0.00018913  0.00615464 -0.01362529 -0.00275093  0.01533716  0.01469282\n",
      " -0.00734659  0.0052854  -0.01663426  0.01241097 -0.00927464 -0.00632821\n",
      "  0.01862271  0.00174677  0.01498141 -0.01214813  0.01032101  0.01984565\n",
      " -0.01691478 -0.01027138 -0.01412967 -0.0097253  -0.00755713 -0.0170724\n",
      "  0.01591121 -0.00968788  0.01684723  0.01052514 -0.01310005  0.00791574\n",
      "  0.0109403  -0.01485307 -0.01481144 -0.00495046 -0.01725145 -0.00316314\n",
      " -0.00080687  0.00659937  0.00288376 -0.00176284 -0.01118812  0.00346073\n",
      " -0.00179474  0.01358738  0.00794718  0.00905894  0.00286861 -0.00539971\n",
      " -0.00873363 -0.00206415]\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Dataset created: \"The elephant trumpets. The snake hisses. The fox yips\"\n",
    "sentences = [[\"the\", \"elephant\", \"trumpets\"],\n",
    "             [\"the\", \"snake\", \"hisses\"],\n",
    "             [\"the\", \"fox\", \"yips\"]]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, workers=1)\n",
    "\n",
    "# Retrieve the embedding for the word \"snake\"\n",
    "fox_vector = model.wv[\"snake\"]\n",
    "\n",
    "# Print the embedding\n",
    "print(\"Embedding for 'snake':\")\n",
    "print(fox_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab87ca1-cb64-4444-9573-256f92be3f8b",
   "metadata": {},
   "source": [
    "### Pretrained GloVe Model (glove-wiki-gigaword-50) Using Gensim\n",
    "\n",
    "* **Retrieving embedding and similar words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "744fda0b-e2c8-43b9-9c88-468c260f03d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding for 'Incredible':\n",
      " [-0.14689   0.54232  -0.011375  0.29162   0.7991   -0.061087  0.44189\n",
      "  0.1891    0.54886   1.1932   -0.26771   0.064008 -0.69519  -0.51787\n",
      "  0.67175  -0.70509   0.51642   0.54056  -0.7944   -0.80188  -0.48548\n",
      "  0.79217  -0.21582  -1.0416    1.2546   -0.27579  -1.4243   -0.06234\n",
      "  1.2305    0.066436  1.7358    0.88321   0.49214  -0.34407  -0.28822\n",
      "  0.43435  -0.24437   0.28734   0.020281 -0.56087  -0.1894   -0.26222\n",
      " -0.47613   0.14826  -0.42779   0.10252   0.22617   0.14601  -0.050739\n",
      "  0.36255 ]\n",
      "Words Similar to 'Incredible':\n",
      " [('amazing', 0.9189565181732178), ('astonishing', 0.8662074208259583), ('awesome', 0.8470885157585144), ('unbelievable', 0.8440187573432922), ('tremendous', 0.8422953486442566)]\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pretrained GloVe model (50-dimensional vectors)\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Retrieve the embedding for the word \"king\"\n",
    "print(\"Word Embedding for 'Incredible':\\n\", glove_model['incredible'])\n",
    "\n",
    "# Find the 5 most similar words to \"king\"\n",
    "print(\"Words Similar to 'Incredible':\\n\", glove_model.most_similar('incredible', topn=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
